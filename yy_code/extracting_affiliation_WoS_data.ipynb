{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Web of Science Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Clean and Filter Affiliations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [],
            "source": [
                "def specific_college_csv(affiliation, short_name, keyword_list, all_keywords = False):\n",
                "    # Read CSV data into pandas dataframes\n",
                "    big_affiliations_df = pd.DataFrame()\n",
                "    big_articles_df = pd.DataFrame()\n",
                "    big_authors_df = pd.DataFrame()\n",
                "    big_keywords_df = pd.DataFrame()\n",
                "    for i in range(1,26):\n",
                "        temp_affiliations_df = pd.read_csv(f\"C:/Users/xieya/Desktop/Spring 2022/VIP VXG/Tableau mini project/Non-GT Data/affiliations/2020_affiliations_{i}.csv\")\n",
                "        articles_df = pd.read_csv(f\"C:/Users/xieya/Desktop/Spring 2022/VIP VXG/Tableau mini project/Non-GT Data/articles/2020_articles_{i}.csv\", index_col=False)\n",
                "        authors_df = pd.read_csv(f\"C:/Users/xieya/Desktop/Spring 2022/VIP VXG/Tableau mini project/Non-GT Data/authors/2020_authors_{i}.csv\")\n",
                "        keywords_df = pd.read_csv(f\"C:/Users/xieya/Desktop/Spring 2022/VIP VXG/Tableau mini project/Non-GT Data/keywords/2020_keywords_{i}.csv\")\n",
                "        \n",
                "        affiliations_df = temp_affiliations_df[temp_affiliations_df.organization == affiliation]\n",
                "        # Redefine affiliations_df to include coauthors of <affiliation> people, even if they aren't a <affiliation> employee\n",
                "        affiliations_df = temp_affiliations_df[temp_affiliations_df[\"docID\"].isin(affiliations_df[\"docID\"])]\n",
                "\n",
                "        # Drop duplicate affiliations\n",
                "        affiliations_df = affiliations_df.drop_duplicates()\n",
                "        affiliations_df = affiliations_df.dropna()\n",
                "\n",
                "        if not all_keywords:\n",
                "            # Limits keywords only from <keyword_list>\n",
                "            keywords_list_df = keywords_df[(keywords_df[\"keyword\"].str.lower()).isin(keyword_list)]\n",
                "            # # Limits affiliations only for those articles in <keyword_list>\n",
                "            affiliations_df = affiliations_df[affiliations_df[\"docID\"].isin(keywords_list_df[\"docID\"])]\n",
                "            # # Limits articles only for those articles in <keyword_list>\n",
                "            articles_df = articles_df[articles_df[\"docID\"].isin(keywords_list_df[\"docID\"])]\n",
                "\n",
                "        # Limit articles to those with author affiliated with <affiliation>\n",
                "        articles_df = articles_df[articles_df[\"docID\"].isin(affiliations_df[\"docID\"])]\n",
                "\n",
                "        # Drop na values\n",
                "        articles_df = articles_df.dropna(subset=[\"title\"])\n",
                "        \n",
                "        # Limit list of authors to those with affiliation to <affiliation>\n",
                "        authors_df = authors_df[authors_df[\"docID\"].isin(affiliations_df[\"docID\"])]\n",
                "        # Limits authors only for those articles in <keyword_list>\n",
                "        authors_df = authors_df[authors_df[\"docID\"].isin(keywords_list_df[\"docID\"])]\n",
                "        # Create dummy columns\n",
                "        authors_df[\"dummy_given_name\"] = authors_df[\"wosFirstName\"]\n",
                "        authors_df[\"dummy_family_name\"] = authors_df[\"wosLastName\"]\n",
                "        authors_df[\"dummy_first_initial\"] = authors_df[\"wosFirstName\"]\n",
                "        # Drop rows with no first or last name\n",
                "        authors_df = authors_df.dropna(subset=[\"wosFirstName\", \"wosLastName\"])\n",
                "        authors_df = authors_df.dropna(subset=[\"wosFullName\"])\n",
                "\n",
                "        ## Standardize all dummy names\n",
                "        # Take first part of name and make lowercase\n",
                "        authors_df[\"dummy_given_name\"] = \\\n",
                "            authors_df[\"dummy_given_name\"].apply(lambda x : x.split(\" \")[0].split(\"-\")[0].lower())\n",
                "\n",
                "        authors_df[\"dummy_family_name\"] = \\\n",
                "            authors_df[\"dummy_family_name\"].apply(lambda x : x.split(\" \")[0].split(\"-\")[0].lower())\n",
                "\n",
                "        # Remove non-alphabetic characters\n",
                "        authors_df[\"dummy_given_name\"] = \\\n",
                "            authors_df[\"dummy_given_name\"].apply(lambda x : \"\".join(filter(str.isalpha, x)))\n",
                "\n",
                "        authors_df[\"dummy_family_name\"] = \\\n",
                "            authors_df[\"dummy_family_name\"].apply(lambda x : \"\".join(filter(str.isalpha, x)))\n",
                "\n",
                "        # Create first initial\n",
                "        authors_df[\"dummy_first_initial\"] = \\\n",
                "            authors_df[\"dummy_first_initial\"].apply(lambda x : x[0].lower())\n",
                "        # Limit keywords to those from <affiliation> papers\n",
                "        keywords_df = keywords_df[keywords_df[\"docID\"].isin(affiliations_df[\"docID\"])]\n",
                "        keywords_df = keywords_df[keywords_df[\"docID\"].isin(keywords_list_df[\"docID\"])]\n",
                "        keywords_df[\"keyword\"] = keywords_df[\"keyword\"].str.lower() #make everything lower case\n",
                "\n",
                "        # Concat all files to big dataframe\n",
                "        big_affiliations_df = pd.concat([big_affiliations_df,affiliations_df], axis=0)\n",
                "        big_articles_df = pd.concat([big_articles_df,articles_df], axis=0)\n",
                "        big_authors_df = pd.concat([big_authors_df,authors_df], axis=0)\n",
                "        big_keywords_df = pd.concat([big_keywords_df,keywords_df], axis=0)\n",
                "\n",
                "    big_affiliations_df.to_csv(f\"C:/Users/xieya/Desktop/Spring 2022/zS2022 EVPR/Web of Science/2020_{short_name}_affiliations.csv\", index=False)\n",
                "    big_articles_df.to_csv(f\"C:/Users/xieya/Desktop/Spring 2022/zS2022 EVPR/Web of Science/2020_{short_name}_articles.csv\", index=False)\n",
                "    big_authors_df.to_csv(f\"C:/Users/xieya/Desktop/Spring 2022/zS2022 EVPR/Web of Science/2020_{short_name}_authors.csv\", index=False)\n",
                "    big_keywords_df.to_csv(f\"C:/Users/xieya/Desktop/Spring 2022/zS2022 EVPR/Web of Science/2020_{short_name}_keywords.csv\", index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [],
            "source": [
                "info_science_keywords = [\"machine learning\", \"artificial intelligence\", \"cybersecurity\", \"cyber security\", \"climate security\",\"environmental security\"]\n",
                "# school = \"North Carolina A&T State University\"\n",
                "# short = \"NCA&T\"\n",
                "\n",
                "schools_dict = {\"North Carolina A&T State University\":\"NCA&T\", \"Prairie View A&M University\":\"PVA&M\", \"Florida A&M University\": \"FA&M\", \n",
                "\"Alabama A&M University\":\"AA&M\", \"Norfolk State Univ\":\"NSU\"}\n",
                "for index, (school, short) in enumerate(schools_dict.items()):\n",
                "    specific_college_csv(affiliation = school, short_name = short, keyword_list = info_science_keywords)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Combining all colleges"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [],
            "source": [
                "master_affiliations_df = pd.DataFrame()\n",
                "master_articles_df = pd.DataFrame()\n",
                "master_authors_df = pd.DataFrame()\n",
                "master_keywords_df = pd.DataFrame()\n",
                "\n",
                "for index, (school, short) in enumerate(schools_dict.items()):\n",
                "    col_affiliations_df = pd.read_csv(f\"C:/Users/xieya/Desktop/Spring 2022/zS2022 EVPR/Web of Science/2020_{short}_affiliations.csv\")\n",
                "    col_articles_df = pd.read_csv(f\"C:/Users/xieya/Desktop/Spring 2022/zS2022 EVPR/Web of Science/2020_{short}_articles.csv\", index_col=False)\n",
                "    col_authors_df = pd.read_csv(f\"C:/Users/xieya/Desktop/Spring 2022/zS2022 EVPR/Web of Science/2020_{short}_authors.csv\")\n",
                "    col_keywords_df = pd.read_csv(f\"C:/Users/xieya/Desktop/Spring 2022/zS2022 EVPR/Web of Science/2020_{short}_keywords.csv\")\n",
                "\n",
                "    master_affiliations_df = pd.concat([master_affiliations_df,col_affiliations_df], axis=0)\n",
                "    master_articles_df = pd.concat([master_articles_df,col_articles_df], axis=0)\n",
                "    master_authors_df = pd.concat([master_authors_df,col_authors_df], axis=0)\n",
                "    master_keywords_df = pd.concat([master_keywords_df,col_keywords_df], axis=0)\n",
                "\n",
                "master_affiliations_df.to_csv(f\"C:/Users/xieya/Desktop/Spring 2022/zS2022 EVPR/Web of Science/master_2020_affiliations.csv\", index=False)\n",
                "master_articles_df.to_csv(f\"C:/Users/xieya/Desktop/Spring 2022/zS2022 EVPR/Web of Science/master_2020_articles.csv\", index=False)\n",
                "master_authors_df.to_csv(f\"C:/Users/xieya/Desktop/Spring 2022/zS2022 EVPR/Web of Science/master_2020_authors.csv\", index=False)\n",
                "master_keywords_df.to_csv(f\"C:/Users/xieya/Desktop/Spring 2022/zS2022 EVPR/Web of Science/master_2020_keywords.csv\", index=False)"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
        },
        "kernelspec": {
            "display_name": "Python 3.8.8 64-bit (conda)",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.8"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
